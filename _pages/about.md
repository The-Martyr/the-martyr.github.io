---
permalink: /
title: "About Me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I'm **Guanyu Zhou**, a 3rd year undergrad interested in artificial intelligence. 

My current research interests are **MLLMs/LLMs (Understanding, Generation, and Reasoning)**, with a current focus on **data**. I hope to learn from MLLMs, reflect on its bottlenecks, and explore the prototype of the next generation of intelligent systems. I did a research internship at the NLPGroup of HKUST(GZ), exploring the perceptual flaws of MLLMs. Prior to this, I had an unforgettable research time at AIMlab, where I worked on problems related to video understanding/action recognition.

My ultimate goal is to build a multimodal, scalable, dataset-free, continuously learning, and self-iterating intelligent system. Feel free to email me to discuss issues in related fields!üòÑ

<br>
# üìÖ News
* [2025.03] I made a sharing at [AI TIME](https://www.bilibili.com/video/BV1YkRAYVEHy/?vd_source=ffc139b7b13aa7195dd7b37795efa6a9), welcome to watch!
* [2025.01] Our paper [CausalMM](https://arxiv.org/abs/2410.04780) has been accepted to **ICLR 2025**, see you in Singapore!
* [2024.07] I am visiting HKUST(GZ) under the supervision of Professor [Xuming Hu](https://xuminghu.github.io)!

<br>
# üìù Publications

<div style="display: flex; align-items: center; margin-bottom: 20px;">
  <div style="flex: 1; padding-left: 0px;">
    <img src="/images/causalmm.png" alt="Flowchart" style="max-width: 100%;">
  </div>
  <div style="width: 60%; padding-left: 10px;">
    <p><strong>[ICLR 2025] Mitigating Modality Prior-Induced Hallucinations in Multimodal Large Language Models via Deciphering Attention Causality</strong></p>
    <p><strong>Guanyu Zhou</strong>, Yibo Yan, Xin Zou, Kun Wang, Aiwei Liu, Xuming Hu</p>
    <a href="https://arxiv.org/pdf/2410.04780" target="_blank"><button>PDF</button></a> 
    <a href="https://github.com/The-Martyr/CausalMM" target="_blank"><button>Code</button></a>
    <a href="https://www.bilibili.com/video/BV1YkRAYVEHy/?vd_source=ffc139b7b13aa7195dd7b37795efa6a9" target="_blank"><button>Talk</button></a> 
    <a href="https://iclr.cc/virtual/2025/poster/30629" target="_blank"><button>ICLR</button></a>
  </div>
</div>

<style>
p {
    margin: 0; /* Remove default margin */
    padding: 0; /* Remove default padding */
}
button {
  background-color: #4CAF50; /* Green background */
  border: none; /* No border */
  color: white; /* White text */
  padding: 10px 20px; /* Padding */
  text-align: center; /* Centered text */
  text-decoration: none; /* No underline */
  display: inline-block; /* Inline block */
  font-size: 16px; /* Font size */
  margin: 4px 2px; /* Margin */
  cursor: pointer; /* Pointer cursor */
  border-radius: 8px; /* Rounded corners */
}
</style>

<div style="display: flex; align-items: center; margin-bottom: 20px;">
  <div style="flex: 1; padding-left: 25px;">
    <img src="/images/OccludeNet.png" alt="Flowchart" style="max-width: 90%;">
  </div>
  <div style="width: 60%; padding-left: 10px;">
    <p><strong>[under review] OccludeNet: A Causal Journey into Mixed-View Actor-Centric Video Action Recognition under Occlusions</strong></p>
    <p><strong>Guanyu Zhou</strong>, Wenxuan Liu, Wenxin Huang, Xuemei Jia, Xian Zhong, Chia-Wen Lin</p>
    <a href="https://arxiv.org/pdf/2411.15729" target="_blank"><button>PDF</button></a> 
    <a href="https://github.com/The-Martyr/OccludeNet-Dataset" target="_blank"><button>Code</button></a>
  </div>
</div>

<style>
p {
    margin: 0; /* Remove default margin */
    padding: 0; /* Remove default padding */
}
button {
  background-color: #4CAF50; /* Green background */
  border: none; /* No border */
  color: white; /* White text */
  padding: 10px 20px; /* Padding */
  text-align: center; /* Centered text */
  text-decoration: none; /* No underline */
  display: inline-block; /* Inline block */
  font-size: 16px; /* Font size */
  margin: 4px 2px; /* Margin */
  cursor: pointer; /* Pointer cursor */
  border-radius: 8px; /* Rounded corners */
}
</style>




<br>
# üìá Experience

<div style="margin-top: 40px; display: flex; align-items: center; margin-bottom: 20px;"> 
  <div style="flex: 1; padding-left: 40px;">
    <img src="/images/68747470733a2f2f686b7573742e6564752e686b2f73697465732f64656661756c742f66696c65732f696d616765732f5553545f4c332e737667.svg" alt="HKUST" style="max-width: 80%; height: auto;">
  </div>
  <div style="flex: 2; padding-left: 10px;">
    <p>[2024.7] <strong>HKUST(GZ) / AI Thrust</strong></p>
    <p>Topic: Bias in Model Prior Knowledge</p>
  </div>
</div>

<div style="display: flex; align-items: center; margin-bottom: 20px;">
  <div style="flex: 1; padding-left: 40px;">
    <img src="/images/AIMLab.jpg" alt="AIMLab" style="max-width: 80%; width: 200px; height: auto;">
  </div>
  <div style="flex: 2; padding-left: 10px;">
    <p>[2023.7] <strong>AIMLab / Action Recogniton Group</strong></p>
    <p>Topic: Bias in the Data (Learning Process)</p>
  </div>
</div>



<br>
# ‚úç Services

### Reviewer

* IEEE International Conference on Computer Vision 2025
* IEEE International Conference on Multimedia & Expo 2025

### Management

* Deputy Director of Competition Management Center of the School's Innovation and Entrepreneurship Department (2023-2024)



<br>
# üë®‚Äçüè´ Talks

* AI TIME Online Pre-Session (ICLR 2025) [[watch on Bilibili]](https://www.bilibili.com/video/BV1YkRAYVEHy/?vd_source=ffc139b7b13aa7195dd7b37795efa6a9)


<br>
# üòò Friends Links
* [hanghang214](https://hanghang214.cn/) Outstanding infosec undergrad, admitted to master's at PKU.

<div style="margin-top: 100px;"></div>

<div style="display:none;">
    <script type="text/javascript" id="mapmyvisitors" src="//mapmyvisitors.com/map.js?d=8MhgTWHJEZzdE82Bb-wBII3RuujWQtydOxS12ZLFdM8&cl=ffffff&w=a"></script>
</div>



